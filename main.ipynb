{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1b5b30fe-615d-4157-b7e9-5f6a390cffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_output_probabilities(training_file, smoothing_delta, output_probs_filename):\n",
    "    # Count occurrences of each (token, tag) pair\n",
    "    token_tag_count = {}\n",
    "    tag_count = {}\n",
    "    with open(training_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                token, tag = line.split('\\t')\n",
    "                token_tag_count[(token, tag)] = token_tag_count.get((token, tag), 0) + 1\n",
    "                tag_count[tag] = tag_count.get(tag, 0) + 1\n",
    "\n",
    "    # Calculate output probabilities\n",
    "    output_probs = {}\n",
    "    unique_tokens = set(token for (token, _) in token_tag_count.keys())\n",
    "    num_unique_tokens = len(unique_tokens)\n",
    "    for (token, tag), count in token_tag_count.items():\n",
    "        output_probs[(token, tag)] = (count + smoothing_delta) / (tag_count[tag] + smoothing_delta * (num_unique_tokens + 1))\n",
    "\n",
    "    # Write output probabilities to file\n",
    "    with open(output_probs_filename, 'w', encoding='utf-8') as f:\n",
    "        for (token, tag), prob in output_probs.items():\n",
    "            f.write(f\"{token}\\t{tag}\\t{prob}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "estimate_output_probabilities('twitter_train.txt', smoothing_delta=0.6, output_probs_filename='naive_output_probs.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c8fc8395-e547-4924-8977-35fea58fe10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_predict(in_output_probs_filename, in_test_filename, out_prediction_filename):\n",
    "    # Load output probabilities from file\n",
    "    output_probs = {}\n",
    "    with open(in_output_probs_filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            token, tag, prob = line.strip().split('\\t')\n",
    "            output_probs[(token, tag)] = float(prob)\n",
    "\n",
    "    # Predict tags for test data\n",
    "    with open(in_test_filename, 'r', encoding='utf-8') as f_in, open(out_prediction_filename, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            token = line.strip()\n",
    "            if token:  # Non-empty line\n",
    "                max_prob = -1\n",
    "                predicted_tag = None\n",
    "                for tag in set(tag for (t, tag) in output_probs.keys() if t == token):\n",
    "                    prob = output_probs.get((token, tag), 0)\n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        predicted_tag = tag\n",
    "                if predicted_tag is not None:\n",
    "                    f_out.write(predicted_tag + '\\n')\n",
    "                else:\n",
    "                    # Handle case when no tags are found for the token\n",
    "                    f_out.write('UNKNOWN\\n')\n",
    "            else:  # Empty line (end of tweet)\n",
    "                f_out.write('\\n')\n",
    "\n",
    "# Example usage:\n",
    "naive_predict('naive_output_probs.txt', 'twitter_dev_no_tag.txt', 'naive_predictions.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "67a235a9-43b6-4729-826f-801fdfe623a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(in_prediction_filename, in_answer_filename):\n",
    "    \"\"\"Do not change this method\"\"\"\n",
    "    with open(in_prediction_filename) as fin:\n",
    "        predicted_tags = [l.strip() for l in fin.readlines() if len(l.strip()) != 0]\n",
    "\n",
    "    with open(in_answer_filename) as fin:\n",
    "        ground_truth_tags = [l.strip() for l in fin.readlines() if len(l.strip()) != 0]\n",
    "\n",
    "    assert len(predicted_tags) == len(ground_truth_tags)\n",
    "    correct = 0\n",
    "    for pred, truth in zip(predicted_tags, ground_truth_tags):\n",
    "        if pred == truth: correct += 1\n",
    "    return correct, len(predicted_tags), correct/len(predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "84a3e4e0-849c-42b2-85a9-434ec3751309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive prediction accuracy:     908/1378 = 0.6589259796806967\n"
     ]
    }
   ],
   "source": [
    "in_ans_filename='twitter_dev_ans.txt'\n",
    "naive_prediction_filename='naive_predictions.txt'\n",
    "correct, total, acc = evaluate(naive_prediction_filename, in_ans_filename)\n",
    "print(f'Naive prediction accuracy:     {correct}/{total} = {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dd9565aa-e17e-4965-b0a8-2ad0730d8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_predict2(in_output_probs_filename, in_train_filename, in_test_filename, out_prediction_filename):\n",
    "    # Load output probabilities from file\n",
    "    output_probs = {}\n",
    "    with open(in_output_probs_filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            token, tag, prob = line.strip().split('\\t')\n",
    "            output_probs[(token, tag)] = float(prob)\n",
    "\n",
    "    # Load tag probabilities from training data\n",
    "    tag_probs = {}\n",
    "    total_tags = 0\n",
    "    with open(in_train_filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                _, tag = line.split('\\t')\n",
    "                tag_probs[tag] = tag_probs.get(tag, 0) + 1\n",
    "                total_tags += 1\n",
    "\n",
    "    # Normalize tag probabilities\n",
    "    for tag in tag_probs:\n",
    "        tag_probs[tag] /= total_tags\n",
    "\n",
    "    # Predict tags for test data\n",
    "    with open(in_test_filename, 'r', encoding='utf-8') as f_in, open(out_prediction_filename, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            token = line.strip()\n",
    "            if token:  # Non-empty line\n",
    "                max_prob = -1\n",
    "                predicted_tag = None\n",
    "                for tag in set(tag for (_, tag) in output_probs.keys()):\n",
    "                    prob = output_probs.get((token, tag), 0) * tag_probs.get(tag, 0)\n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        predicted_tag = tag\n",
    "                f_out.write(predicted_tag + '\\n')\n",
    "            else:  # Empty line (end of tweet)\n",
    "                f_out.write('\\n')\n",
    "\n",
    "# Example usage:\n",
    "naive_predict2('naive_output_probs.txt', 'twitter_train.txt', 'twitter_dev_no_tag.txt', 'naive_predictions2.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "722e1d30-7aba-4b14-8e99-b50eee9e1a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 909/1378 = 65.97%\n"
     ]
    }
   ],
   "source": [
    "correct, total, accuracy = evaluate('naive_predictions2.txt', 'twitter_dev_ans.txt')\n",
    "print(f'Accuracy: {correct}/{total} = {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "de183693-898a-403a-8830-c59393fa47a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_probabilities(training_file, smoothing_delta, trans_probs_filename):\n",
    "    # Count occurrences of each (previous tag, current tag) pair\n",
    "    transition_count = {}\n",
    "    tag_count = {}\n",
    "    \n",
    "    # Initialize tag_count to account for START tag\n",
    "    tag_count['*'] = 0\n",
    "\n",
    "    with open(training_file, 'r', encoding='utf-8') as f:\n",
    "        prev_tag = '*'\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                token, tag = line.split('\\t')\n",
    "                tag_count[tag] = tag_count.get(tag, 0) + 1\n",
    "                transition_count[(prev_tag, tag)] = transition_count.get((prev_tag, tag), 0) + 1\n",
    "                prev_tag = tag\n",
    "            else:  # Empty line (end of tweet)\n",
    "                tag_count['*'] += 1  # Increment count for STOP tag\n",
    "                prev_tag = '*'  # Reset prev_tag for next tweet\n",
    "\n",
    "    # Smooth transition counts\n",
    "    num_tags = len(tag_count)\n",
    "    for prev_tag in tag_count:\n",
    "        for tag in tag_count:\n",
    "            transition_count[(prev_tag, tag)] = (transition_count.get((prev_tag, tag), 0) + smoothing_delta) / \\\n",
    "                                                (tag_count.get(prev_tag, 0) + smoothing_delta * num_tags)\n",
    "\n",
    "    # Write transition probabilities to file\n",
    "    with open(trans_probs_filename, 'w', encoding='utf-8') as f:\n",
    "        for (prev_tag, tag), prob in transition_count.items():\n",
    "            f.write(f\"{prev_tag}\\t{tag}\\t{prob}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "compute_transition_probabilities('twitter_train.txt', smoothing_delta=0.1, trans_probs_filename='trans_probs.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "11052675-99d4-431f-9557-e10b52973f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_predict(in_tags_filename, in_trans_probs_filename, in_output_probs_filename, in_test_filename,\n",
    "                    out_predictions_filename):\n",
    "    # Load tags\n",
    "    with open(in_tags_filename, 'r', encoding='utf-8') as f:\n",
    "        tags = [line.strip() for line in f]\n",
    "\n",
    "    # Load transition probabilities\n",
    "    trans_probs = {}\n",
    "    with open(in_trans_probs_filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            prev_tag, tag, prob = line.strip().split('\\t')\n",
    "            trans_probs[(prev_tag, tag)] = float(prob)\n",
    "\n",
    "    # Load output probabilities\n",
    "    output_probs = {}\n",
    "    with open(in_output_probs_filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            token, tag, prob = line.strip().split('\\t')\n",
    "            output_probs[(token, tag)] = float(prob)\n",
    "\n",
    "    # Viterbi algorithm\n",
    "    with open(in_test_filename, 'r', encoding='utf-8') as f_in, open(out_predictions_filename, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            tokens = line.strip().split()\n",
    "            if tokens:  # Non-empty line\n",
    "                n = len(tokens)\n",
    "                best_scores = {}\n",
    "                back_pointers = {}\n",
    "                for tag in tags:\n",
    "                    # Initialization\n",
    "                    best_scores[(0, tag)] = trans_probs.get(('*', tag), 0) * output_probs.get((tokens[0], tag), 0)\n",
    "                    back_pointers[(0, tag)] = None\n",
    "\n",
    "                for i in range(1, n):\n",
    "                    for tag in tags:\n",
    "                        best_score, back_pointer = max(\n",
    "                            ((best_scores[(i - 1, prev_tag)] * trans_probs.get((prev_tag, tag), 0) *\n",
    "                              output_probs.get((tokens[i], tag), 0), prev_tag) for prev_tag in tags)\n",
    "                        )\n",
    "                        best_scores[(i, tag)] = best_score\n",
    "                        back_pointers[(i, tag)] = back_pointer\n",
    "\n",
    "                # Find the best final tag\n",
    "                best_final_tag = max(tags, key=lambda tag: best_scores[(n - 1, tag)])\n",
    "\n",
    "                # Trace back to find the best tag sequence\n",
    "                predicted_tags = [best_final_tag]\n",
    "                prev_tag = best_final_tag\n",
    "                for i in range(n - 1, 0, -1):\n",
    "                    prev_tag = back_pointers[(i, prev_tag)]\n",
    "                    predicted_tags.insert(0, prev_tag)\n",
    "\n",
    "                # Write predicted tags to output file\n",
    "                for token, tag in zip(tokens, predicted_tags):\n",
    "                    f_out.write(tag + '\\n')\n",
    "            else:  # Empty line (end of tweet)\n",
    "                f_out.write('\\n')\n",
    "\n",
    "# Example usage:\n",
    "viterbi_predict('twitter_tags.txt', 'trans_probs.txt', 'naive_output_probs.txt', 'twitter_dev_no_tag.txt', 'viterbi_predictions.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "672954c9-7a34-4b40-a24b-79fc55a8f554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 949/1378 = 68.87%\n"
     ]
    }
   ],
   "source": [
    "# Example usage to calculate accuracy\n",
    "correct, total, accuracy = evaluate('viterbi_predictions.txt', 'twitter_dev_ans.txt')\n",
    "print(f'Accuracy: {correct}/{total} = {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a1618d46-28cb-41a9-a83e-182a1296bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_probabilities_with_smoothing(training_file, smoothing_delta):\n",
    "    # Count occurrences of each (previous tag, current tag) pair\n",
    "    transition_count = {}\n",
    "    tag_count = {}\n",
    "    \n",
    "    # Initialize tag_count to account for START tag\n",
    "    tag_count['*'] = 0\n",
    "\n",
    "    with open(training_file, 'r', encoding='utf-8') as f:\n",
    "        prev_tag = '*'\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                _, tag = line.split('\\t')\n",
    "                tag_count[tag] = tag_count.get(tag, 0) + 1\n",
    "                transition_count[(prev_tag, tag)] = transition_count.get((prev_tag, tag), 0) + 1\n",
    "                prev_tag = tag\n",
    "            else:  # Empty line (end of tweet)\n",
    "                tag_count['*'] += 1  # Increment count for STOP tag\n",
    "                prev_tag = '*'  # Reset prev_tag for next tweet\n",
    "\n",
    "    # Smooth transition counts\n",
    "    num_tags = len(tag_count)\n",
    "    for prev_tag in tag_count:\n",
    "        for tag in tag_count:\n",
    "            transition_count[(prev_tag, tag)] = (transition_count.get((prev_tag, tag), 0) + smoothing_delta) / \\\n",
    "                                                (tag_count.get(prev_tag, 0) + smoothing_delta * num_tags)\n",
    "\n",
    "    # Write transition probabilities to file\n",
    "    with open('trans_probs2.txt', 'w', encoding='utf-8') as f:\n",
    "        for (prev_tag, tag), prob in transition_count.items():\n",
    "            f.write(f\"{prev_tag}\\t{tag}\\t{prob}\\n\")\n",
    "\n",
    "\n",
    "def compute_output_probabilities_with_smoothing(training_file, smoothing_delta):\n",
    "    # Count occurrences of each (token, tag) pair\n",
    "    output_count = {}\n",
    "    tag_count = {}\n",
    "\n",
    "    with open(training_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                token, tag = line.split('\\t')\n",
    "                tag_count[tag] = tag_count.get(tag, 0) + 1\n",
    "                output_count[(token, tag)] = output_count.get((token, tag), 0) + 1\n",
    "\n",
    "    # Smooth output counts\n",
    "    vocabulary_size = len(output_count)  # Number of unique (token, tag) pairs\n",
    "    num_tags = len(tag_count)\n",
    "    for token, tag in output_count:\n",
    "        output_count[(token, tag)] = (output_count.get((token, tag), 0) + smoothing_delta) / \\\n",
    "                                     (tag_count.get(tag, 0) + smoothing_delta * vocabulary_size)\n",
    "\n",
    "    # Write output probabilities to file\n",
    "    with open('output_probs2.txt', 'w', encoding='utf-8') as f:\n",
    "        for (token, tag), prob in output_count.items():\n",
    "            f.write(f\"{token}\\t{tag}\\t{prob}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "compute_transition_probabilities_with_smoothing('twitter_train.txt', smoothing_delta=0.1)\n",
    "compute_output_probabilities_with_smoothing('twitter_train.txt', smoothing_delta=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e5c02541-2d17-4758-9d36-469d24adeba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_predict2(in_tags_filename, in_trans_probs_filename, in_output_probs_filename, in_test_filename,\n",
    "                     out_predictions_filename):\n",
    "    # Load tags\n",
    "    with open(in_tags_filename, 'r', encoding='utf-8') as f:\n",
    "        tags = [line.strip() for line in f]\n",
    "\n",
    "    # Compute transition probabilities with Laplace smoothing\n",
    "    compute_transition_probabilities_with_smoothing('twitter_train.txt', smoothing_delta=3)\n",
    "\n",
    "    # Compute output probabilities with Laplace smoothing\n",
    "    compute_output_probabilities_with_smoothing('twitter_train.txt', smoothing_delta=0.1)\n",
    "\n",
    "    # Load transition probabilities with Laplace smoothing\n",
    "    trans_probs = {}\n",
    "    with open('trans_probs2.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            prev_tag, tag, prob = line.strip().split('\\t')\n",
    "            trans_probs[(prev_tag, tag)] = float(prob)\n",
    "\n",
    "    # Load output probabilities with Laplace smoothing\n",
    "    output_probs = {}\n",
    "    with open('output_probs2.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            token, tag, prob = line.strip().split('\\t')\n",
    "            output_probs[(token, tag)] = float(prob)\n",
    "\n",
    "    # Apply Viterbi algorithm with improved probabilities\n",
    "    with open(in_test_filename, 'r', encoding='utf-8') as f_in, open(out_predictions_filename, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            tokens = line.strip().split()\n",
    "            if tokens:  # Non-empty line\n",
    "                n = len(tokens)\n",
    "                best_scores = {}\n",
    "                back_pointers = {}\n",
    "                for tag in tags:\n",
    "                    # Initialization\n",
    "                    best_scores[(0, tag)] = trans_probs.get(('*', tag), 0) * output_probs.get((tokens[0], tag), 0)\n",
    "                    back_pointers[(0, tag)] = None\n",
    "\n",
    "                for i in range(1, n):\n",
    "                    for tag in tags:\n",
    "                        best_score, back_pointer = max(\n",
    "                            ((best_scores[(i - 1, prev_tag)] * trans_probs.get((prev_tag, tag), 0) *\n",
    "                              output_probs.get((tokens[i], tag), 0), prev_tag) for prev_tag in tags)\n",
    "                        )\n",
    "                        best_scores[(i, tag)] = best_score\n",
    "                        back_pointers[(i, tag)] = back_pointer\n",
    "\n",
    "                # Find the best final tag\n",
    "                best_final_tag = max(tags, key=lambda tag: best_scores[(n - 1, tag)])\n",
    "\n",
    "                # Trace back to find the best tag sequence\n",
    "                predicted_tags = [best_final_tag]\n",
    "                prev_tag = best_final_tag\n",
    "                for i in range(n - 1, 0, -1):\n",
    "                    prev_tag = back_pointers[(i, prev_tag)]\n",
    "                    predicted_tags.insert(0, prev_tag)\n",
    "\n",
    "                # Write predicted tags to output file\n",
    "                for token, tag in zip(tokens, predicted_tags):\n",
    "                    f_out.write(tag + '\\n')\n",
    "            else:  # Empty line (end of tweet)\n",
    "                f_out.write('\\n')\n",
    "\n",
    "# Example usage:\n",
    "viterbi_predict2('twitter_tags.txt', 'trans_probs.txt', 'output_probs.txt', 'twitter_dev_no_tag.txt', 'viterbi_predictions2.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dd2aada4-3183-4233-84a7-641575f338ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 956/1378 = 69.38%\n"
     ]
    }
   ],
   "source": [
    "# Example usage to calculate accuracy\n",
    "correct, total, accuracy = evaluate('viterbi_predictions2.txt', 'twitter_dev_ans.txt')\n",
    "print(f'Accuracy: {correct}/{total} = {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e5fbb1-d693-42c5-a2e7-30c294c5702f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
